{
    "sourceFile": "newton.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1671762747974,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1671762747974,
            "name": "Commit-0",
            "content": "import numpy as np\n\ndef newton(fp, fpp, x0, tol=1e-5, maxiter=100000):\n\t\"\"\"\n\tfp: function that takes an input x and returns the gradient of f at x\n\tfpp: function that takes an input x and returns the Hessian of f at x\n\tx0: initial point\n\ttol: toleracne parameter in the stopping crieterion. Newton's method stops \n\t     when the 2-norm of the gradient is smaller than tol\n\tmaxiter: maximum number of iterations\n\n\tThis function should return a list of the sequence of approximate solutions\n\tx_k produced by each iteration\n\t\"\"\"\n\tx_traces = [np.array(x0)]\n\tx = np.array(x0)\n\t#   START OF YOUR CODE\n\n\tpass\n\n\t#\tEND OF YOUR CODE\n\n\treturn x_traces \n\ndef damped_newton(f, fp, fpp, x0, alpha=0.5, beta=0.5, tol=1e-5, maxiter=100000):\n\t\"\"\"\n\tf: function that takes an input x an returns the value of f at x\n\tfp: function that takes an input x and returns the gradient of f at x\n\tfpp: function that takes an input x and returns the Hessian of f at x\n\tx0: initial point in gradient descent\n\talpha: parameter in Armijo's rule \n\t\t\t\tf(x + t * d) > f(x) + t * alpha * <f'(x), d>\n\tbeta: constant factor used in stepsize reduction\n\ttol: toleracne parameter in the stopping crieterion. Here we stop\n\t     when the 2-norm of the gradient is smaller than tol\n\tmaxiter: maximum number of iterations in gradient descent.\n\n\tThis function should return a list of the sequence of approximate solutions\n\tx_k produced by each iteration and the total number of iterations in the inner loop\n\t\"\"\"\n\tx_traces = [np.array(x0)]\n\tstepsize_traces = []\n\ttot_num_iter = 0\n\t\n\tx = np.array(x0)\n\n\tfor it in range(maxiter):\n\t\t#   START OF YOUR CODE\n\t\t\n\t\tpass\n\n\t\t#\tEND OF YOUR CODE\n\t\tx_traces.append(np.array(x))\n\t\tstepsize_traces.append(stepsize)\n\n\treturn x_traces, stepsize_traces, tot_num_iter\n\ndef newton_eq(f, fp, fpp, x0, A, b, initial_stepsize=1.0, alpha=0.5, beta=0.5, tol=1e-8, maxiter=100000):\n\t\"\"\"\n\tf: function that takes an input x an returns the value of f at x\n\tfp: function that takes an input x and returns the gradient of f at x\n\tfpp: function that takes an input x and returns the Hessian of f at x\n\tA, b: constraint A x = b\n\tx0: initial feasible point\n\tinitial_stepsize: initial stepsize used in backtracking line search\n\talpha: parameter in Armijo's rule \n\t\t\t\tf(x + t * d) > f(x) + t * alpha * f(x) @ d\n\tbeta: constant factor used in stepsize reduction\n\ttol: toleracne parameter in the stopping crieterion. Gradient descent stops \n\t     when the 2-norm of the Newton direction is smaller than tol\n\tmaxiter: maximum number of iterations in outer loop of damped Newton's method.\n\n\tThis function should return a list of the iterates x_k\n\t\"\"\"\n\tx_traces = [np.array(x0)]\n\t\n\tm = len(b)\n\tx = np.array(x0)\n\n\tfor it in range(maxiter):\n\t#   START OF YOUR CODE\n\t\n\t\ta = np.block([\n\t\t\t[fpp(x),      A.T                              ],\n\t\t\t[A,           np.zeros((m,m))]\n\t\t])\n\n\n\t\tk = np.transpose(np.block([-fp(x), np.zeros((m))])) #这里要小心！不要把zeros开成多维的了！\n\n\t\td = np.linalg.solve(a,k)[:A.shape[1]]\n\n\t\tstepsize = initial_stepsize\n\t\twhile(f(x + stepsize*d) > f(x) + alpha * stepsize * fp(x).T @ d):\n\t\t\tstepsize = beta * stepsize\n\n\n\t\tx = x + stepsize * d\n\t\tx_traces.append(x)\n\n\t\tif(np.linalg.norm(d, ord = 2) < tol):\n\t\t\tbreak;\n\t#\tEND OF YOUR CODE\n\n\treturn x_traces"
        }
    ]
}