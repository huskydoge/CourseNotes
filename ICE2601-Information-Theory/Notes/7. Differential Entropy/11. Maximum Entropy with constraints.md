#信息论 
- Let the random variable $X \in R$ have mean $\mu$ and variance $\sigma^2$. Then
$$
h(X) \leq \frac{1}{2} \log 2 \pi e \sigma^2
$$
with equality iff $X \sim \mathcal{N}\left(\mu, \sigma^2\right)$

* Let the random variable $X \in R$ satisfy $E X^2 \leq \sigma^2$. Then
$$
h(X) \leq \frac{1}{2} \log 2 \pi e \sigma^2
$$
with equality iff $X \sim \mathcal{N}\left(0, \sigma^2\right)$
1. Let $X_G \sim \mathcal{N}\left(\mu, \sigma^2\right)$. Consider
$$
D\left(X \| X_G\right) \geq 0 \quad \text { 相对熵大于等于0 }
$$
Then
$$
\begin{gathered}
\int f \log \frac{f}{g} \geq 0 \\
h(X)=h(f) \leq-\int f \log g=-\int f \log \frac{1}{\sqrt{2 \pi \sigma^2}}+f\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) \\
h(X) \leq \frac{1}{2} \log 2 \pi \sigma^2+\frac{1}{2}=\frac{1}{2} \log 2 \pi e \sigma^2
\end{gathered}
$$
2. $\operatorname{Var}(X)=E\left(X^2\right)-E(X)^2 \leq \sigma^2 . \Rightarrow$ Case 1 .

>[!summary]
>$$
\boldsymbol{E}\left(\boldsymbol{X}^2\right), \operatorname{Var}(\mathbf{X}) \text { 给定的情况下,高斯分布最大化微分樀 }
>$$


## Maximum Entropy

Consider the following problem: Maximize the entropy $h(f)$ over all probability densities $f$ satisfying conditions below<mark style="background: #BBFABBA6;"> (denoted as ++)</mark>
1. $f(x) \geq 0$, with equality outside the support
2. $\int_s f(x) d x=1$
3. $\int_S f(x) r_i(x) d x=\alpha_i$ for $1 \leq i \leq m .\left(r_i(x)\right.$ is a function of $\left.x\right)$


Thus, $f$ is a density on support set $S$ <mark style="background: #FFB86CA6;">meeting certain moment constraints $\alpha_1, \alpha_2, \ldots, \alpha_m$</mark>.

>[!Note] Theorem 12.1.1 (Maximum entropy distribution) 
Let
>$$
>f^*(x)=f_\lambda(x)=e^{\lambda_0+\sum_{i=1}^m \lambda_i r_i(x)}
>$$
>$x \in S$, where $\lambda_0, \ldots, \lambda_m$ are chosen so that $f^*$ satisfies $(++)$. Then $f^*$ uniquely maximizes $h(f)$ over all probability densities $f$ satisfying constraints $(++)$
- Let $S=[a, b]$, with no other constraints. Then the maximum entropy distribution is the uniform distribution over this range.

* $S=[0, \infty)$ and $E X=\mu$. Then the entropy-maximizing distribution is

$$
f(x)=\frac{1}{\mu} e^{-\frac{x}{\mu}}, \quad x \geq 0
$$
* $S=(-\infty, \infty), E X=\alpha_1$, and $E X^2=\alpha_2$. The maximum entropy distribution is $\mathcal{N}\left(\alpha_1, \alpha_2-\alpha_1^2\right)$