#信息论 

>[!warning]
>Differential entropy does not serve as <mark style="background: #FFB86CA6;">a measure of the average amount of information</mark> contained in a continuous random variable.

In fact, a continuous random variable generally contains an infinite amount of information
Let $X$ be uniformly distributed on $[0,1)$. Then we can write
$$
X=0 . X_1 X_2, \ldots
$$
The <mark style="background: #FFB86CA6;">dyadic(二进制的)</mark> expansion of $X$, where $X_i^{\prime} s$ is a sequence of i.i.d bits.

Then
$$
\begin{aligned}
H(X) & =H\left(X_1, X_2, \ldots\right) \\
& =\sum_{i=1}^{\infty} H\left(X_i\right) \\
& =\sum_{i=1}^{\infty} 1 \\
& =\infty
\end{aligned}
$$