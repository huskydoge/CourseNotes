#信息论 
>[!danger] Differences between inequalities of the discrete entropy and differential entropy

* Both $H(X, Y) \leq H(X)+H(Y)$ and $h(X, Y) \leq h(X)+h(Y)$ are valid
* $H(X, Y) \geq H(X)$ but neither $h(X, Y) \geq h(X)$ nor $h(X, Y) \leq h(X)$ is valid

>[!example] Take $H(X, Y, Z) \leq \frac{1}{4} H(X)+\frac{1}{2} H(Y, Z)+\frac{3}{4} H(Z, X)$ for example.

Count the weights of random variables $X, Y, Z$ in both sides
$$
X: 1,1 ; Y: 1, \frac{1}{2} ; Z: 1, \frac{5}{4}
$$
The net weights of $X, Y, Z$ are $0, \frac{1}{2},-\frac{1}{4}$


>[!Note] Balanced: If the net weights of $X, Y, Z$ are all zero.
$$
h(X, Y) \leq h(X)+h(Y) \text { and } h(X, Y, Z) \leq \frac{1}{2} h(X, Y)+\frac{1}{2} h(Y, Z)+\frac{1}{2} h(Z, X)
$$
Let $[n]:=\{1,2, \ldots, n\}$. For any $\alpha \subseteq[n]$, denote $\left(X_i: i \in \alpha\right)$ by $X_\alpha$. For example, $\alpha=\{1,3,4\}$, we denote $X_1, X_3, X_4$ by $X_{\{1,3,4\}}$ for simplicity.

We could write any information inequality in the form $\sum_\alpha w_\alpha H\left(X_\alpha\right) \geq 0$ or $\sum_\alpha w_\alpha h\left(X_\alpha\right) \geq 0$.

>[!note] An information inequality is called balanced if for any $i \in[n]$, the net weight of $X_i$ is zero.

The linear continuous inequality $\sum_\alpha w_\alpha h\left(X_\alpha\right) \geq 0$ is valid<mark style="background: #CACFD9A6;"> if and only if its corresponding discrete counterpart $\sum_\alpha w_\alpha H\left(X_\alpha\right) \geq 0$ is valid and balanced.</mark>



>[!cite] 
>Balanced Information Inequalities, T. H. Chan, IEEE Transactions on Information Theory, Vol. 49, No. 12, December 2003
