#信息论 

The differential entropy of a set $X_1, X_2, \ldots, X_n$ of random variables with density $f\left(x_1, x_2, \ldots, x_n\right)$ is defined as
$$
h\left(X_1, X_2, \ldots, X_n\right)=-\int f\left(x^n\right) \log f\left(x^n\right) d x^n
$$
>[!warning] Note $dx^n$ here! Actually can be $dxdydz\cdots$


If $X, Y$ have a joint density function $f(x, y)$, we can define the conditional differential entropy $h(X \mid Y)$ as
$$
\begin{gathered}
h(X \mid Y)=-\int f(x,y) \log f(x \mid y) d x d y  \\ \\
h(X\mid Y)=h(X, Y)-h(Y)
\end{gathered}
$$
>$h(X \mid Y) \leq h(X)$, with equality iff $X$ and $Y$ are independent.


>[!Note] (Chain rule for differential entropy)
>$$
>h\left(X_1, X_2, \ldots, X_n\right)=\sum_{i=1}^n h\left(X_i \mid X_1, X_2, \ldots, X_{i-1}\right)
>$$
>$h\left(X_1, X_2, \ldots, X_n\right) \leq \sum_{i=1}^n h\left(X_i\right)$
>with equality iff $X_1, X_2, \ldots, X_n$ are independent.