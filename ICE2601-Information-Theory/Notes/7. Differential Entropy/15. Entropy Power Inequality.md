#信息论 

>[!Note] Shannon 1948, Entropy power inequality (EPI)

If $X$ and $Y$ are independent random $n$-vectors with densities, then
$$
\begin{aligned}
& e^{\frac{2}{n} h(X+Y)} \geq e^{\frac{2}{n} h(X)}+e^{\frac{2}{n} h(Y)} \\
& e^{2 h(X+Y)} \geq e^{2 h(X)}+e^{2 h(Y)}(n=1)
\end{aligned}
$$

>[!note] Fisher information inequality (FII)
$$
\frac{1}{I(X+Y)} \geq \frac{1}{I(X)}+\frac{1}{I(Y)}
$$

>Most profound result in Shannon's 1948 paper

EPI can imply some very fundamental results
- Uncertainty principle
- Young's inequality
- Nash's inequality
- Cramer-Rao bound


>[!cite] Reference
>
1. T. Cover, ‘‘Information theoretic inequalities,’’ 1990
2. O. Rioul, “Information Theoretic Proofs of Entropy Power Inequalities,” 2011
