#信息论 

>[!Note] (Mutual information between correlated Gaussian random variables with correlation $\rho$ ) 
Let $(X, Y) \sim \mathcal{N}(0, K)$, where
>$$
>K=\left[\begin{array}{cc}
>\sigma^2 & \rho \sigma^2 \\
\rho \sigma^2 & \sigma^2
\end{array}\right]
>$$


$I(X ; Y) ?$
$$
\begin{gathered}
h(X)=h(Y)=\frac{1}{2} \log 2 \pi e \sigma^2 \\
h(X, Y)=\frac{1}{2} \log (2 \pi e)^2|K|=\frac{1}{2} \log (2 \pi e)^2 \sigma^4\left(1-\rho^2\right) \\
I(X ; Y)=h(X)+h(Y)-h(X, Y)=-\frac{1}{2} \log \left(1-\rho^2\right)
\end{gathered}
$$

* $\rho=0, X$ and $Y$ are independent and $I$ is 0
* $\rho= \pm 1, X$ and $Y$ are perfectly correlated and $I$ is $\infty$