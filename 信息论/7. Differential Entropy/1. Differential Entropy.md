#信息论 #概率论 

## Definition

Let $X$ be a random variable with cumulative distribution function (累积分布函数)
$$
F(x)=\operatorname{Pr}(X \leq x)
$$
- If $F(x)$ is continuous, the random variable is said to be continuous

Let $f(x)=F^{\prime}(x)$ when the derivative is defined. If $\int_{-\infty}^{\infty} f(x)=1, f(x)$ is called the <mark style="background: #FF5582A6;">probability density function</mark> for $X$.
- The set where $f(x)>0$ is called the <mark style="background: #FF5582A6;">support set of</mark> $X$.
The differential entropy $h(X)$ of a continuous random variable $X$ with density $f(x)$ is defined as
$$
h(\mathbb{X})=-\int_S f(x) \log f(x) d x
$$
where $S$ is the support set of the random variable. (概率密度要大于0！)


>The differential entropy is sometimes written as $h(f)$ rather than $h(X)$

 >[!Note] $h(X+c)=h(X)$ (Translation does not change the differential entropy)
$$
\begin{aligned}
& \boldsymbol{p}(\boldsymbol{x}) \Rightarrow \boldsymbol{f}(\boldsymbol{x}) \\
& \sum \Rightarrow \int^{H(X)} \\
& \boldsymbol{H}(\boldsymbol{X})
\end{aligned}
$$

>[!warning]
$H(X)$ is always non-negative. $h(X)$ may be negative。我们可以取f为$\delta$函数
>
>这是因为概率密度函数是可以大于1。和离散的情况不同，它不再代表某点的概率大小。连续情况下概率由积分定义。概率密度函数要求积分为1！


## Example
Consider a random variable distributed uniformly from 0 to a, then $\boldsymbol{h}(X)=\log a$
Let $X \sim \mathcal{N}\left(\mu, \sigma^2\right)$, then $h(X)=\frac{1}{2} \log 2 \pi e \sigma^2$
- When $X$ is uniformly distributed in $[0, a]$,
$$
f(x)=\frac{1}{a}, \quad h(X)=-\int_0^a \frac{1}{a} \log \frac{1}{a} d x=\log a
$$
- When $X$ is Gaussian $\mathcal{N}\left(\mu, \sigma^2\right)$, then
$$
\begin{gathered}
f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \\
h(f(x))=-\int f(x) \log f(x) d x
\end{gathered}
$$
这里用了以 $\mathrm{e}$ 为底的 $\log \quad=-\int f(x) \log \frac{1}{\sqrt{2 \pi \sigma^2}}+f(x)\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) d x$
$$
\begin{gathered}
\int f(x) d x=1 \text { and } \operatorname{Var}(X)=\int_0(x-\mu)^2 f(x) d x=\sigma^2 \\
h(f(x))=\frac{1}{2} \log 2 \pi \sigma^2+\frac{1}{2}=\frac{1}{2} \log 2 \pi e \sigma^2 \quad \text { Mean and Variance }
\end{gathered}
$$