#信息论 

>[!Note] The relative entropy (or Kullback-Leibler distance) 


$D(f \| g$ ) between two densities $f$ and $g$ is defined by
$$
D(f \| g)=\int f \log \frac{f}{g}
$$

***
The mulual information $I(X ; Y)$ between two random variables with joint density $f(x, y)$ is defined as
$$
I(X ; Y)=\int f(x, y) \log \frac{f(x, y)}{f(x) f(y)} d x d y
$$
$$
\begin{gathered}
I(X ; Y)=h(X)-h(X \mid Y)=h(Y)-h(Y \mid X)=h(X)+h(Y)-h(X, Y) \\
I(X ; Y)=D(f(x, y) \| f(x) f(y))
\end{gathered}
$$
$$
D(f \| g) \geq 0
$$
with equality iff $f=g$ almost everywhere (a.e.).
>[!Note]
$I(X ; Y) \geq 0$, with equality iff $X$ and $Y$ are independent.


## Mutual Information: Master Definition
The mutual information between two random variables is the limit of the mutual information between their quantized versions
$$
\begin{aligned}
I\left(X^{\Delta} ; Y^{\Delta}\right) & =H\left(X^{\Delta}\right)-H\left(X^{\Delta} \mid Y^{\Delta}\right) \\
& \approx h(X)-\log \Delta-(h(x \mid y)-\log \Delta) \\
& =I(X ; Y)
\end{aligned}
$$
>[!Note] Definition. The mutual information between two random variables $X$ and $Y$
>$$
>I(X ; Y)=\sup _{\mathcal{P}, \mathcal{Q}} I\left([X]_{\mathcal{P}} ;[Y]_Q\right)
>$$
>where the supremum is over all finite partitions $\mathcal{P}$ and $\mathcal{Q}$
- Let $\mathcal{X}$ be the range of a random variable $X$. A partition $\mathcal{P}$ of $\mathcal{X}$ is a finite collection of disjoint sets $P_i$ such that $\cup_i P_i=\mathcal{X}$. The quantization of $X$ by $\mathcal{P}$ (denoted $[X]_{\mathcal{P}}$ ) is the discrete random variable defined by
$$
\operatorname{Pr}\left([X]_P=i\right)=\operatorname{Pr}\left(X \in P_i\right)=\int_{P_i} d F(x)
$$
For two random variables $X$ and $Y$ with partitions $\mathcal{P}$ and $Q$, we can calculate the mutual information between the quantized versions of $X$ and $Y$


>This is the master definition of mutual information that always applies, even to joint distributions with atoms, densities, and singular parts.